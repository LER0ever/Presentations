title: P3、PipeDream、GPipe - Large-Scale Distributed Training Modern Approaches
speaker: 荣懿 <i@rongyi.io>
url: https://rongyi.io
prismTheme: solarizedlight
js:
    - https://www.echartsjs.com/asset/theme/infographic.js
plugins:
    - echarts: {theme: infographic}
    - mermaid: {theme: forest}
    - katex

<slide class="bg-apple aligncenter">

# P3、PipeDream、GPipe {.text-shadow}

Large-Scale Distributed Training - Modern Approaches {.text-intro.animated.fadeInUp.delay-500 .text-shadow}

<br /><br /><br /><br /><br /><br />

> 荣懿 \<i@rongyi.io\>

Link to this presentation ```https://p.rongyi.io/190809```

<!--[:fa-link: Github](https://github.com/ksky521/nodeppt){.button.ghost.animated.flipInX.delay-1200}-->

<slide class="size-60 aligncenter text-apple">

## Background

---

### 1. Training Process {.text-shadow .animated.fadeInUp}  
### 2. Data Parallelism (_input partitioning_) {.text-shadow .animated.fadeInUp}  
### 3. Model Parallelism (_network structure partitioning_) {.text-shadow .animated.fadeInUp}  
### 4. Pipelining (_layer partitioning_) {.text-shadow .animated.fadeInUp}  
### 5. Hybrid Pipelining {.text-shadow .animated.fadeInUp}  


<slide :class="aligncenter text-shadow text-apple">

# The Training Process
## DNN Architecture

---

![DNN Architecture](./images/DNN-Arch.png)

<slide :class="aligncenter text-shadow text-apple">

# The Training Process
## Forward Evaluation & Backpropagation
![Fwd Back](./images/FwdBack.png)

---

![DNN Architecture](./images/DNN-Arch.png)


<slide :class="aligncenter text-apple">

# Data Parallelism {.text-shadow}
:::column {.vertical-align}

#### The **data** is partitioned across multiple GPUs
#### Each GPU has a full copy of the model     
#### Train on its own weight
#### Synchronize with other GPUs

---

![Data Parallel](./images/DataParallel.png)

<slide :class="text-apple">

# Problems with Data Parallelism {.text-shadow .aligncenter}
:::column {.vertical-align}

## Sync Strategy
#### Bulk Synchronization Parallel (BSP)
#### Asynchronous Parallel (ASP)

## Bottlenecks
#### Batch Normalization & Weight Updates

---

![Data Parallel Problem](./images/DataParallel-Problem.png)

---

## Finer-Grained Data Parallelism
#### Pipelined Execution
#### Asynchronous Execution 
#### Decompose Minibatch

<slide :class="text-apple">

# Improving Data Parallelism {.text-shadow .aligncenter}
:::column {.vertical-align}


![op1](./images/DP-Opt1.png)

---

![op2](./images/DP-Opt2.png)

---

![op3](./images/DP-Opt3.png)

### Convolution Decomposition


<slide :class="aligncenter text-apple">

# Model Parallelism {.text-shadow}
:::column {.vertical-align}

#### The **model** is partitioned across multiple GPUs
#### Each GPU train only a portion of the model   
#### Saves GPU memory with only a partial model
#### Requires communication after each layer

---

![Model Parallel](./images/ModelParallel.png)


<slide :class="text-apple">

# Problems with Model Parallelism {.text-shadow .aligncenter}
:::column {.vertical-align}

## GPU Under-Utilization

## Data Partitioning Point Solution

## Communication after every layer
#### Fully connected layer (FC) requires an all-to-all communication

---

![Model Parallel Problem](./images/ModelParallel-Problem.png)


<slide :class="aligncenter text-apple">

# Pipelining {.text-shadow}
:::column {.vertical-align}

## Variant 1
#### Overlapping computation between one layer and the next
##### Forward Evaluation
##### BackPropagation
##### Weight updates

## Variant 2
#### Partition DNN according to depth and assign layers to different GPUs


---

![Pipelining](./images/Pipelining.png)


<slide :class="text-apple">

# Pipelining Var.2 (Layer partitioning) {.text-shadow .aligncenter}
:::column {.vertical-align}

## Pros
#### 1. No need to store full parameters on every GPU
#### 2. Communication pattern is fixed
#### 3. Layers at each GPU are fixed, allowing weight caching to decrease memory round-trips

---

## Cons
#### 1. Data has to arrive at a specific speed in order to fully utilize the system
#### 2. Latency proportionally increase with number of GPUs



<slide class="bg-apple aligncenter">
# Priority-Based Parameter Propagation

## For Distributed DNN Training

---

### A.K.A. P3

--- 

University of British Columbia & Vector Institute

<slide :class="text-apple">

# Observations {.text-shadow .aligncenter}
:::column {.vertical-align}

## 1. Temporal gap between data generated and consumed

## 2. Finer sub-layer-level granularity improves network utilization {.aligntop}
#### Heavy models with skewed weight size
##### VGG16, VGG19
##### Sockeye

---



![P3-1](./images/P3-1.png)

<slide :class="text-apple">

# 1. Priority-based Propagation {.text-shadow .aligncenter}
:::column {.vertical-align}

![P3-4-1](./images/P3-4-1.png)

---


![P3-4-2](./images/P3-4-2.png)


<slide :class="text-apple">

# 2. Parameter Slicing {.text-shadow .aligncenter}
:::column {.vertical-align}

![P3-6-1](./images/P3-6-1.png)

---


![P3-6-2](./images/P3-6-2.png)





<slide class="bg-apple aligncenter">
# PipeDream

## Fast and Efficient Pipeline Parallel DNN Training


--- 

Carnegie Mellon University  
Microsoft Research  
Stanford University  

<slide :class="text-apple size-60">

# Observations {.text-shadow .aligncenter}
:::column {.vertical-align}

## 1. Data Parallelism BSP scenarios
![Data Parallel Problem](./images/DataParallel-Problem.png)


---

## 2. Model Parallelism: only 1 GPU at work


![Model Parallel Problem](./images/ModelParallel-Problem.png)


<slide :class="text-apple size-50">

# Observations {.text-shadow .aligncenter}
:::shadowbox

# Pipeline Parallelism
a combination of Data-, Model- Parallelism and Pipelining

---

# Layer Partitioning
The partitioning is done after a profiling run, so as to make sure each stage gets the similar amount of work, and data communicated across stages are minimized.

---

# Work Scheduling

:::







<slide class="bg-black-blue text-apple aligncenter" image="https://rongyi.io/wp-content/uploads/2019/05/angelwing.jpg .dark">

## Thank You {.aligncenter}
:::column {.vertical-align}

#### References {aligncenter}

[1]: Dean, Jeffrey, et al. “Large scale distributed deep networks.”  
[2]: Saliya Ekanayake. “Model Parallelism in Deep Learning is NOT What You Think”  

---

[:fa-github: Source code of this presentation](https://github.com/ksky521/nodeppt){.button.animated.delay-1s.fadeInUp .alignright}
