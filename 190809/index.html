<!doctype html><html><head><meta charset=UTF-8><title>P3、PipeDream、GPipe - Large-Scale Distributed Training Modern Approaches - By 荣懿 &lt;i@rongyi.io&gt;</title><link rel=stylesheet href=//cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css><link rel=stylesheet href=//cdn.staticfile.org/prism/1.15.0/themes/prism.min.css><link rel=stylesheet href=//cdn.staticfile.org/KaTeX/0.10.0-rc.1/katex.min.css><link rel=stylesheet href=//cdn.staticfile.org/prism/1.15.0/themes/prism-solarizedlight.min.css><link rel=stylesheet href=//cdn.staticfile.org/KaTeX/0.5.1/katex.min.css><link href=./css/chunk-vendors.b65b960b.css rel=stylesheet></head><body><div><article id=webslides><section slide class="slide bg-apple aligncenter"><div class=wrap wrap=true><h1 class=text-shadow>P3、PipeDream、GPipe</h1><p class="text-intro animated fadeInUp delay-500 text-shadow">Large-Scale Distributed Training - Modern Approaches</p><br><br><br><br><br><br><blockquote><p>荣懿 &lt;i@rongyi.io&gt;</p></blockquote><p>Link to this presentation <code>https://p.rongyi.io/190809</code></p></div></section><section slide class="slide size-60 aligncenter text-apple"><div class=wrap wrap=true><h2>Background</h2><hr><h3 class="text-shadow animated fadeInUp">1. Training Process</h3><h3 class="text-shadow animated fadeInUp">2. Data Parallelism (<em>input partitioning</em>)</h3><h3 class="text-shadow animated fadeInUp">3. Model Parallelism (<em>network structure partitioning</em>)</h3><h3 class="text-shadow animated fadeInUp">4. Pipelining (<em>layer partitioning</em>)</h3><h3 class="text-shadow animated fadeInUp">5. Hybrid Pipelining</h3></div></section><section slide class=slide :class="aligncenter text-shadow text-apple"><div class="wrap aligncenter text-shadow text-apple" wrap=true><h1>The Training Process</h1><h2>DNN Architecture</h2><hr><p><img src=./img/DNN-Arch.000a6ef3.png alt="DNN Architecture"></p></div></section><section slide class=slide :class="aligncenter text-shadow text-apple"><div class="wrap aligncenter text-shadow text-apple" wrap=true><h1>The Training Process</h1><h2>Forward Evaluation &amp; Backpropagation</h2><p><img src=./img/FwdBack.13a9084b.png alt="Fwd Back"></p><hr><p><img src=./img/DNN-Arch.000a6ef3.png alt="DNN Architecture"></p></div></section><section slide class=slide :class="aligncenter text-apple"><div class="wrap aligncenter text-apple" wrap=true><h1 class=text-shadow>Data Parallelism</h1><div class="vertical-align grid"><div class=column><h4>The <strong>data</strong> is partitioned across multiple GPUs</h4><h4>Each GPU has a full copy of the model</h4><h4>Train on its own weight</h4><h4>Synchronize with other GPUs</h4></div><div class=column><p><img src=./img/DataParallel.7a758d56.png alt="Data Parallel"></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">Problems with Data Parallelism</h1><div class="vertical-align grid"><div class=column><h2>Sync Strategy</h2><h4>Bulk Synchronization Parallel (BSP)</h4><h4>Asynchronous Parallel (ASP)</h4><h2>Bottlenecks</h2><h4>Batch Normalization &amp; Weight Updates</h4></div><div class=column><p><img src=./img/DataParallel-Problem.01edccb8.png alt="Data Parallel Problem"></p></div><div class=column><h2>Finer-Grained Data Parallelism</h2><h4>Pipelined Execution</h4><h4>Asynchronous Execution</h4><h4>Decompose Minibatch</h4></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">Improving Data Parallelism</h1><div class="vertical-align grid"><div class=column><p><img src=./img/DP-Opt1.593d8e31.png alt=op1></p></div><div class=column><p><img src=./img/DP-Opt2.e2c15749.png alt=op2></p></div><div class=column><p><img src=./img/DP-Opt3.64920b65.png alt=op3></p><h3>Convolution Decomposition</h3></div></div></div></section><section slide class=slide :class="aligncenter text-apple"><div class="wrap aligncenter text-apple" wrap=true><h1 class=text-shadow>Model Parallelism</h1><div class="vertical-align grid"><div class=column><h4>The <strong>model</strong> is partitioned across multiple GPUs</h4><h4>Each GPU train only a portion of the model</h4><h4>Saves GPU memory with only a partial model</h4><h4>Requires communication after each layer</h4></div><div class=column><p><img src=./img/ModelParallel.d57e3a34.png alt="Model Parallel"></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">Problems with Model Parallelism</h1><div class="vertical-align grid"><div class=column><h2>GPU Under-Utilization</h2><h2>Data Partitioning Point Solution</h2><h2>Communication after every layer</h2><h4>Fully connected layer (FC) requires an all-to-all communication</h4></div><div class=column><p><img src=./img/ModelParallel-Problem.fe1613df.png alt="Model Parallel Problem"></p></div></div></div></section><section slide class=slide :class="aligncenter text-apple"><div class="wrap aligncenter text-apple" wrap=true><h1 class=text-shadow>Pipelining</h1><div class="vertical-align grid"><div class=column><h2>Variant 1</h2><h4>Overlapping computation between one layer and the next</h4><h5>Forward Evaluation</h5><h5>BackPropagation</h5><h5>Weight updates</h5><h2>Variant 2</h2><h4>Partition DNN according to depth and assign layers to different GPUs</h4></div><div class=column><p><img src=./img/Pipelining.5eb1ead9.png alt=Pipelining></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">Pipelining Var.2 (Layer partitioning)</h1><div class="vertical-align grid"><div class=column><h2>Pros</h2><h4>1. No need to store full parameters on every GPU</h4><h4>2. Communication pattern is fixed</h4><h4>3. Layers at each GPU are fixed, allowing weight caching to decrease memory round-trips</h4></div><div class=column><h2>Cons</h2><h4>1. Data has to arrive at a specific speed in order to fully utilize the system</h4><h4>2. Latency proportionally increase with number of GPUs</h4></div></div></div></section><section slide class="slide bg-apple aligncenter"><div class=wrap wrap=true><h1>Priority-Based Parameter Propagation</h1><h2>For Distributed DNN Training</h2><hr><h3>A.K.A. P3</h3><hr><p>University of British Columbia &amp; Vector Institute</p></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">Observations</h1><div class="vertical-align grid"><div class=column><h2>1. Temporal gap between data generated and consumed</h2><h2 class=aligntop>2. Finer sub-layer-level granularity improves network utilization</h2><h4>Heavy models with skewed weight size</h4><h5>VGG16, VGG19</h5><h5>Sockeye</h5></div><div class=column><p><img src=./img/P3-1.23e64beb.png alt=P3-1></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">1. Priority-based Propagation</h1><div class="vertical-align grid"><div class=column><p><img src=./img/P3-4-1.bc876179.png alt=P3-4-1></p></div><div class=column><p><img src=./img/P3-4-2.688dc186.png alt=P3-4-2></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">2. Parameter Slicing</h1><div class="vertical-align grid"><div class=column><p><img src=./img/P3-6-1.33522233.png alt=P3-6-1></p></div><div class=column><p><img src=./img/P3-6-2.be9f8b8c.png alt=P3-6-2></p></div></div></div></section><section slide class="slide bg-apple aligncenter"><div class=wrap wrap=true><h1>PipeDream</h1><h2>Fast and Efficient Pipeline Parallel DNN Training</h2><hr><p>Carnegie Mellon University<br>Microsoft Research<br>Stanford University</p></div></section><section slide class=slide :class="text-apple size-60"><div class="wrap text-apple size-60" wrap=true><h1 class="text-shadow aligncenter">Observations</h1><div class="vertical-align grid"><div class=column><h2>1. Data Parallelism BSP scenarios</h2><p><img src=./img/DataParallel-Problem.01edccb8.png alt="Data Parallel Problem"></p></div><div class=column><h2>2. Model Parallelism:: only 1 GPU at work</h2><p><img src=./img/ModelParallel-Problem.fe1613df.png alt="Model Parallel Problem"></p></div></div></div></section><section slide class=slide :class="text-apple size-50"><div class="wrap text-apple size-50" wrap=true><h1 class="text-shadow aligncenter">Observations</h1><div class="bg-white shadow"><ul class="flexblock reasons"><li><h1>Pipeline Parallelism</h1><p>a combination of Data-, Model- Parallelism and Pipelining</p></li><li><h1>Layer Partitioning</h1><p>The partitioning is done after a profiling run, so as to make sure each stage gets the similar amount of work, and data communicated across stages are minimized.</p></li><li><h1>Work Scheduling</h1></li></ul></div></div></section><section slide class="slide bg-black-blue text-apple aligncenter" image="https://rongyi.io/wp-content/uploads/2019/05/angelwing.jpg .dark"><span class="background dark" style="background-image:url('https://rongyi.io/wp-content/uploads/2019/05/angelwing.jpg')"></span><div class=wrap wrap=true><h2 class=aligncenter>Thank You</h2><div class="vertical-align grid"><div class=column><h4 aligncenter="">References</h4><p>[1]:: Dean, Jeffrey, et al. “Large scale distributed deep networks.”<br>[2]:: Saliya Ekanayake. “Model Parallelism in Deep Learning is NOT What You Think”</p></div><div class=column><p><a href=https://github.com/ksky521/nodeppt class="button animated delay-1s fadeInUp alignright" target=_blank><i class="fa fa-github"></i> Source code of this presentation</a></p></div></div></div></section></article></div><script src=//cdn.staticfile.org/echarts/4.1.0-release/echarts.min.js></script><script src=//cdn.staticfile.org/mermaid/8.0.0/mermaid.min.js></script><script>mermaid.startOnLoad = false;</script><script src=https://www.echartsjs.com/asset/theme/infographic.js></script><script>window.pluginsOptions = {"echarts":{"theme":"infographic"},"mermaid":{"theme":"forest"}}



document.addEventListener('DOMContentLoaded', () => {
    const ws = new WebSlides({
        loop: false
    })
    window.wsInstance = ws;
}, false)</script><script src=./js/chunk-vendors.32049ce8.js></script><script src=./js/190809.6f8ac235.js></script></body></html>