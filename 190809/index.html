<!doctype html><html><head><meta charset=UTF-8><title>P3、PipeDream、GPipe - Large-Scale Distributed Training Modern Approaches - By 荣懿</title><link rel=stylesheet href=//cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css><link rel=stylesheet href=//cdn.staticfile.org/prism/1.15.0/themes/prism.min.css><link rel=stylesheet href=//cdn.staticfile.org/KaTeX/0.10.0-rc.1/katex.min.css><link rel=stylesheet href=//cdn.staticfile.org/prism/1.15.0/themes/prism-solarizedlight.min.css><link rel=stylesheet href=//cdn.staticfile.org/KaTeX/0.5.1/katex.min.css><link href=./css/chunk-vendors.ede7c604.css rel=stylesheet></head><body><div><article id=webslides><section slide class="slide bg-apple aligncenter"><div class=wrap wrap=true><h1 class=text-shadow>P3、PipeDream、GPipe</h1><p class="text-intro animated fadeInUp delay-500 text-shadow">Large-Scale Distributed Training - Modern Approaches</p><br><br><br><br><br><br><blockquote><p>荣懿 <a href=https://rongyi.io target=_blank>&lt;https:://rongyi.io&gt;</a></p></blockquote><p>Link to this presentation <a href=https://p.rongyi.io/190809 target=_blank><code>https://p.rongyi.io/190809</code></a><br>Contact Me <a href=mailto:i@rongyi.io target=_blank><code>i@rongyi.io</code></a> or <a href=mailto:rongyi.ry@alibaba-inc.com target=_blank><code>rongyi.ry@alibaba-inc.com</code></a></p></div></section><section slide class="slide size-60 aligncenter text-apple"><div class=wrap wrap=true><h2>Background</h2><hr><h3 class="text-shadow animated fadeInUp">1. Training Process</h3><h3 class="text-shadow animated fadeInUp">2. Data Parallelism (<em>input partitioning</em>)</h3><h3 class="text-shadow animated fadeInUp">3. Model Parallelism (<em>network structure partitioning</em>)</h3><h3 class="text-shadow animated fadeInUp">4. Pipelining (<em>layer partitioning</em>)</h3><h3 class="text-shadow animated fadeInUp">5. Hybrid Pipelining</h3></div></section><section slide class=slide :class="aligncenter text-shadow text-apple"><div class="wrap aligncenter text-shadow text-apple" wrap=true><h1>The Training Process</h1><h2>DNN Architecture</h2><hr><p><img src=./img/DNN-Arch.000a6ef3.png alt="DNN Architecture"></p></div></section><section slide class=slide :class="aligncenter text-shadow text-apple"><div class="wrap aligncenter text-shadow text-apple" wrap=true><h1>The Training Process</h1><h2>Forward Evaluation &amp; Backpropagation</h2><p><img src=./img/FwdBack.13a9084b.png alt="Fwd Back"></p></div></section><section slide class=slide :class="aligncenter text-apple"><div class="wrap aligncenter text-apple" wrap=true><h1 class=text-shadow>Data Parallelism</h1><div class="vertical-align grid"><div class=column><h4>The <strong>data</strong> is partitioned across multiple GPUs</h4><h4>Each GPU has a full copy of the model</h4><h4>Train on its own weight</h4><h4>Synchronize with other GPUs</h4></div><div class=column><p><img src=./img/DataParallel.7a758d56.png alt="Data Parallel"></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">Problems with Data Parallelism</h1><div class="vertical-align grid"><div class=column><h2>Sync Strategy</h2><h4>Bulk Synchronization Parallel (BSP)</h4><h4>Asynchronous Parallel (ASP)</h4><h2>Bottlenecks</h2><h4>Batch Normalization &amp; Weight Updates</h4></div><div class=column><p><img src=./img/DataParallel-Problem.01edccb8.png alt="Data Parallel Problem"></p></div><div class=column><h2>Finer-Grained Data Parallelism</h2><h4>Pipelined Execution</h4><h4>Asynchronous Execution</h4><h4>Decompose Minibatch</h4></div></div></div></section><section slide class=slide :class="aligncenter text-apple"><div class="wrap aligncenter text-apple" wrap=true><h1 class=text-shadow>Model Parallelism</h1><div class="vertical-align grid"><div class=column><h4>The <strong>model</strong> is partitioned across multiple GPUs</h4><h4>Each GPU train only a portion of the model</h4><h4>Saves GPU memory with only a partial model</h4><h4>Requires communication after each layer</h4></div><div class=column><p><img src=./img/ModelParallel.d57e3a34.png alt="Model Parallel"></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">Problems with Model Parallelism</h1><div class="vertical-align grid"><div class=column><h2>GPU Under-Utilization</h2><h2>Data Partitioning Point Solution</h2><h2>Communication after every layer</h2><h4>Fully connected layer (FC) requires an all-to-all communication</h4></div><div class=column><p><img src=./img/ModelParallel-Problem.fe1613df.png alt="Model Parallel Problem"></p></div></div></div></section><section slide class=slide :class="aligncenter text-apple"><div class="wrap aligncenter text-apple" wrap=true><h1 class=text-shadow>Pipelining</h1><div class="vertical-align grid"><div class=column><h2>Variant 1</h2><h4>Overlapping computation between one layer and the next</h4><h5>Forward Evaluation</h5><h5>BackPropagation</h5><h5>Weight updates</h5><h2>Variant 2</h2><h4>Partition DNN according to depth and assign layers to different GPUs</h4></div><div class=column><p><img src=./img/Pipelining.5eb1ead9.png alt=Pipelining></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">Pipelining Var.2 (Layer partitioning)</h1><div class="vertical-align grid"><div class=column><h2>Pros</h2><h4>1. No need to store full parameters on every GPU</h4><h4>2. Communication pattern is fixed</h4><h4>3. Layers at each GPU are fixed, allowing weight caching to decrease memory round-trips</h4></div><div class=column><h2>Cons</h2><h4>1. Data has to arrive at a specific speed in order to fully utilize the system</h4><h4>2. Latency proportionally increase with number of GPUs</h4></div></div></div></section><section slide class="slide bg-apple aligncenter"><div class=wrap wrap=true><h1>Priority-Based Parameter Propagation</h1><h2>For Distributed DNN Training</h2><hr><h3>A.K.A. P3</h3><hr><p>University of British Columbia &amp; Vector Institute</p></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">Observations</h1><div class="vertical-align grid"><div class=column><h2>1. Temporal gap between data generated and consumed</h2><h2 class=aligntop>2. Finer sub-layer-level granularity improves network utilization</h2><h4>Heavy models with skewed weight size</h4><h5>VGG16, VGG19</h5><h5>Sockeye</h5></div><div class=column><p><img src=./img/P3-1.23e64beb.png alt=P3-1></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">1. Priority-based Propagation</h1><div class="vertical-align grid"><div class=column><p><img src=./img/P3-4-1.bc876179.png alt=P3-4-1></p></div><div class=column><p><img src=./img/P3-4-2.688dc186.png alt=P3-4-2></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class="text-shadow aligncenter">2. Parameter Slicing</h1><div class="vertical-align grid"><div class=column><p><img src=./img/P3-6-1.33522233.png alt=P3-6-1></p></div><div class=column><p><img src=./img/P3-6-2.be9f8b8c.png alt=P3-6-2></p></div></div></div></section><section slide class=slide :class="text-apple aligncenter size-70"><div class="wrap text-apple aligncenter size-70" wrap=true><h1>Applications on VGG16</h1><div class="vertical-align grid"><div class=column><p><img src=./img/VGG-Stats.465ba2c6.png alt="VGG Stats"></p></div><div class=column><p><img src=./img/VGG-Layer-Dist.2921a312.png alt="VGG Layer Stats"> <img src=./img/VGG-Time-Dist.9747c574.png alt="VGG Time Stats"></p></div></div></div></section><section slide class=slide :class="text-apple aligncenter"><div class="wrap text-apple aligncenter" wrap=true><h1>Applications on VGG16</h1><h3>Gap between 1st and 2nd Conv2D</h3><p><img src=./img/VGG-1m8g-Timeline.119be81c.png alt="VGG Timeline"></p></div></section><section slide class=slide :class="text-apple aligncenter"><div class="wrap text-apple aligncenter" wrap=true><h1>Applications on VGG16</h1><h3>Gap between FE and BP</h3><p><img src=./img/VGG-1m8g-FBGap.97509d1d.png alt="VGG Timeline"></p></div></section><section slide class=slide :class="text-apple aligncenter"><div class="wrap text-apple aligncenter" wrap=true><h1>P3 Simulation w/ VGG data</h1><div class="vertical-align grid"><div class=column><pre class=language-go><code class=language-go><span class="token keyword">func</span> <span class="token function">P3Simulation</span><span class="token punctuation">(</span>ds <span class="token operator">*</span>DataStats<span class="token punctuation">,</span> ms <span class="token operator">*</span>ModelStats<span class="token punctuation">)</span> <span class="token builtin">float64</span> <span class="token punctuation">{</span>
	<span class="token comment">// Data Preparation</span>
	curts <span class="token operator">:=</span> <span class="token number">0.0</span> <span class="token comment">// current timestamp</span>
	<span class="token keyword">for</span> i <span class="token operator">:=</span> <span class="token function">len</span><span class="token punctuation">(</span>ms<span class="token punctuation">.</span>Layers<span class="token punctuation">)</span> <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">;</span> i <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">--</span> <span class="token punctuation">{</span>
		m <span class="token operator">:=</span> ms<span class="token punctuation">.</span>Layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
		<span class="token keyword">var</span> db DataBlock
		db<span class="token punctuation">.</span>Size <span class="token operator">=</span> m<span class="token punctuation">.</span>ParamNum <span class="token operator">*</span> <span class="token number">4</span>
		db<span class="token punctuation">.</span>ETA <span class="token operator">=</span> <span class="token function">float64</span><span class="token punctuation">(</span>db<span class="token punctuation">.</span>Size<span class="token punctuation">)</span> <span class="token operator">/</span> Bandwidth
		fmt<span class="token punctuation">.</span><span class="token function">Printf</span><span class="token punctuation">(</span><span class="token string">"ETA for %d: %f\n"</span><span class="token punctuation">,</span> i<span class="token punctuation">,</span> db<span class="token punctuation">.</span>ETA<span class="token punctuation">)</span>
		curts <span class="token operator">+=</span> m<span class="token punctuation">.</span>BackTime
		db<span class="token punctuation">.</span>AvailableTime <span class="token operator">=</span> curts
		ds<span class="token punctuation">.</span>Blocks <span class="token operator">=</span> <span class="token function">append</span><span class="token punctuation">(</span>ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">,</span> db<span class="token punctuation">)</span>
	<span class="token punctuation">}</span>
	n <span class="token operator">:=</span> <span class="token function">len</span><span class="token punctuation">(</span>ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">)</span>
	<span class="token keyword">for</span> i <span class="token operator">:=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> n<span class="token punctuation">;</span> i<span class="token operator">++</span> <span class="token punctuation">{</span>
		b <span class="token operator">:=</span> <span class="token operator">&amp;</span>ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token punctuation">]</span>
		fmt<span class="token punctuation">.</span><span class="token function">Printf</span><span class="token punctuation">(</span><span class="token string">"%f - %f\n"</span><span class="token punctuation">,</span> b<span class="token punctuation">.</span>AvailableTime<span class="token punctuation">,</span> curts<span class="token punctuation">)</span>
		b<span class="token punctuation">.</span>StdAvailableTime <span class="token operator">=</span> b<span class="token punctuation">.</span>AvailableTime <span class="token operator">-</span> curts <span class="token comment">// - b.AvailableTime</span>
	<span class="token punctuation">}</span>
	curts <span class="token operator">=</span> <span class="token number">0.0</span>
	<span class="token keyword">for</span> i<span class="token punctuation">,</span> m <span class="token operator">:=</span> <span class="token keyword">range</span> ms<span class="token punctuation">.</span>Layers <span class="token punctuation">{</span>
		curts <span class="token operator">+=</span> m<span class="token punctuation">.</span>FwdTime
		ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>NeedTime <span class="token operator">=</span> curts
		ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>CompTime <span class="token operator">=</span> m<span class="token punctuation">.</span>FwdTime
	<span class="token punctuation">}</span>
	fmt<span class="token punctuation">.</span><span class="token function">Printf</span><span class="token punctuation">(</span><span class="token string">"DS: %v\n"</span><span class="token punctuation">,</span> <span class="token operator">*</span>ds<span class="token punctuation">)</span>
</code></pre></div><div class=column><pre class=language-go><code class=language-go>	<span class="token comment">// Sequential Simulation</span>
	ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>DriftBack <span class="token operator">+=</span> ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>ETA
	ds<span class="token punctuation">.</span>Offset <span class="token operator">+=</span> ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>n<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>DriftBack

	<span class="token keyword">for</span> i <span class="token operator">:=</span> n <span class="token operator">-</span> <span class="token number">2</span><span class="token punctuation">;</span> i <span class="token operator">>=</span> <span class="token number">0</span><span class="token punctuation">;</span> i<span class="token operator">--</span> <span class="token punctuation">{</span>
		prevSlot <span class="token operator">:=</span> ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>AvailableTime <span class="token operator">-</span> ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>AvailableTime
		afterSlot <span class="token operator">:=</span> ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">.</span>CompTime
		<span class="token keyword">if</span> prevSlot<span class="token operator">+</span>afterSlot <span class="token operator">&lt;</span> ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>ETA <span class="token punctuation">{</span> <span class="token comment">// need to further drift back</span>
			<span class="token keyword">if</span> ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>ETA<span class="token operator">-</span><span class="token punctuation">(</span>prevSlot<span class="token operator">+</span>afterSlot<span class="token punctuation">)</span> <span class="token operator">&lt;</span> ds<span class="token punctuation">.</span>Excess <span class="token punctuation">{</span> <span class="token comment">// try to use Excess first</span>
				fmt<span class="token punctuation">.</span><span class="token function">Printf</span><span class="token punctuation">(</span><span class="token string">"Excess before use: %f\n"</span><span class="token punctuation">,</span> ds<span class="token punctuation">.</span>Excess<span class="token punctuation">)</span>
				ds<span class="token punctuation">.</span>Excess <span class="token operator">-=</span> ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>ETA <span class="token operator">-</span> <span class="token punctuation">(</span>prevSlot <span class="token operator">+</span> afterSlot<span class="token punctuation">)</span>
				fmt<span class="token punctuation">.</span><span class="token function">Printf</span><span class="token punctuation">(</span><span class="token string">"Excess after use: %f\n"</span><span class="token punctuation">,</span> ds<span class="token punctuation">.</span>Excess<span class="token punctuation">)</span>
			<span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
				fmt<span class="token punctuation">.</span><span class="token function">Printf</span><span class="token punctuation">(</span><span class="token string">"Excess before use: %f\n"</span><span class="token punctuation">,</span> ds<span class="token punctuation">.</span>Excess<span class="token punctuation">)</span>
				ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>DriftBack <span class="token operator">=</span> ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>ETA <span class="token operator">-</span> <span class="token punctuation">(</span>prevSlot <span class="token operator">+</span> afterSlot<span class="token punctuation">)</span> <span class="token operator">-</span> ds<span class="token punctuation">.</span>Excess
				ds<span class="token punctuation">.</span>Offset <span class="token operator">+=</span> ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>DriftBack
				ds<span class="token punctuation">.</span>Excess <span class="token operator">=</span> <span class="token number">0</span>
				fmt<span class="token punctuation">.</span><span class="token function">Printf</span><span class="token punctuation">(</span><span class="token string">"Excess after use: %f\n"</span><span class="token punctuation">,</span> ds<span class="token punctuation">.</span>Excess<span class="token punctuation">)</span>
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span> <span class="token keyword">else</span> <span class="token punctuation">{</span>
			ds<span class="token punctuation">.</span>Excess <span class="token operator">+=</span> prevSlot <span class="token operator">+</span> afterSlot <span class="token operator">-</span> ds<span class="token punctuation">.</span>Blocks<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>ETA
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
	fmt<span class="token punctuation">.</span><span class="token function">Printf</span><span class="token punctuation">(</span><span class="token string">"Final Offset: %f\n"</span><span class="token punctuation">,</span> ds<span class="token punctuation">.</span>Offset<span class="token punctuation">)</span>
	fmt<span class="token punctuation">.</span><span class="token function">Printf</span><span class="token punctuation">(</span><span class="token string">"Pure Computational Time: %f\n"</span><span class="token punctuation">,</span> ms<span class="token punctuation">.</span>AllCompTime<span class="token punctuation">)</span>
	<span class="token keyword">return</span> ds<span class="token punctuation">.</span>Offset
<span class="token punctuation">}</span>
</code></pre></div></div></div></section><section slide class="slide bg-apple aligncenter"><div class=wrap wrap=true><h1>PipeDream</h1><h2>Fast and Efficient Pipeline Parallel DNN Training</h2><hr><p>Carnegie Mellon University<br>Microsoft Research<br>Stanford University</p></div></section><section slide class=slide :class="text-apple size-60"><div class="wrap text-apple size-60" wrap=true><h1 class="text-shadow aligncenter">Observations</h1><div class="vertical-align grid"><div class=column><h2>1. Data Parallelism BSP scenarios</h2><p><img src=./img/DataParallel-Problem.01edccb8.png alt="Data Parallel Problem"></p></div><div class=column><h2>2. Model Parallelism:: only 1 GPU at work</h2><p><img src=./img/ModelParallel-Problem.fe1613df.png alt="Model Parallel Problem"></p></div></div></div></section><section slide class=slide :class="text-apple size-50"><div class="wrap text-apple size-50" wrap=true><h1 class="text-shadow aligncenter">PipeDream's Approach</h1><div class="bg-white shadow"><ul class="flexblock reasons"><li><h1>Pipeline Parallelism</h1><p>a combination of Data-, Model- Parallelism and Pipelining</p></li><li><h1>Layer Partitioning</h1><p>The partitioning is done after a profiling run, so as to make sure each stage gets the similar amount of work, and data communicated across stages are minimized.</p></li><li><h1>Work Scheduling</h1><p>Pipelining w/ <em>one-forward-one-backward</em>(1F1B) to make sure forward is progressing in each minibatch. Round-robin load balancing for Data Parallelism.</p></li></ul></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><div class="vertical-align grid"><div class=column><h1>1. Pipeline Parallelism</h1><h2>Observation from Google</h2><h4>Data Parallelism on Convolutional Layers</h4><h4>Model Parallelism on Fully-Connected Layers</h4><p>[4]:: One weird trick for parallelizing convolutional neural networks - Google</p><h2>with PipeDream</h2><h4>Data Parallelism on large Convolutional Layers</h4><h4>Pipelined Model Parallelism on the entire net</h4></div><div class=column><p><img src=./img/PD-6.4f41e2c4.png alt=PD-6></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><div class="vertical-align grid"><div class=column><h1>2. Layer Partitioning</h1><h3>During the profiling</h3><h5>the following data is recorded for each layer l</h5><ul><li><span class=katex><span class=katex-mathml><math><semantics><mrow><msub><mi>T</mi><mi>l</mi></msub></mrow><annotation encoding=application/x-tex>T_l</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.68333em;></span><span class="strut bottom" style=height:0.83333em;vertical-align:-0.15em;></span><span class="base textstyle uncramped"><span class=mord><span class="mord mathit" style=margin-right:0.13889em;>T</span><span class=vlist><span style=top:0.15em;margin-right:0.05em;margin-left:-0.13889em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style=margin-right:0.01968em;>l</span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span></span></span></span>:: total computation time across forward and backward pass for the layer</li><li><span class=katex><span class=katex-mathml><math><semantics><mrow><msub><mi>a</mi><mi>l</mi></msub></mrow><annotation encoding=application/x-tex>a_l</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.43056em;></span><span class="strut bottom" style=height:0.58056em;vertical-align:-0.15em;></span><span class="base textstyle uncramped"><span class=mord><span class="mord mathit">a</span><span class=vlist><span style=top:0.15em;margin-right:0.05em;margin-left:0em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style=margin-right:0.01968em;>l</span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span></span></span></span>:: the size of the output activations of the layer, and the size of input gradients in the backward pass.</li><li><span class=katex><span class=katex-mathml><math><semantics><mrow><msub><mi>w</mi><mi>l</mi></msub></mrow><annotation encoding=application/x-tex>w_l</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.43056em;></span><span class="strut bottom" style=height:0.58056em;vertical-align:-0.15em;></span><span class="base textstyle uncramped"><span class=mord><span class="mord mathit" style=margin-right:0.02691em;>w</span><span class=vlist><span style=top:0.15em;margin-right:0.05em;margin-left:-0.02691em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style=margin-right:0.01968em;>l</span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span></span></span></span>:: the size of parameters for layer <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>l</mi></mrow><annotation encoding=application/x-tex>l</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.69444em;></span><span class="strut bottom" style=height:0.69444em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.01968em;>l</span></span></span></span></li></ul></div><div class=column><p><img src=./img/PD-7.e4a4a905.png alt=PD-7></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class=aligncenter>2. Layer Partitioning</h1><h3 class=aligncenter>Partitioning Algorithm</h3><div class="vertical-align grid"><div class=column><h3>Notations</h3><h5>Train a DNN with <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>N</mi></mrow><annotation encoding=application/x-tex>N</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.68333em;></span><span class="strut bottom" style=height:0.68333em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.10903em;>N</span></span></span></span> layers across <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=application/x-tex>M</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.68333em;></span><span class="strut bottom" style=height:0.68333em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.10903em;>M</span></span></span></span> available machines</h5><h5><span class=katex><span class=katex-mathml><math><semantics><mrow><mi>A</mi><mo>(</mo><mi>j</mi><mo separator=true>,</mo><mi>m</mi><mo>)</mo></mrow><annotation encoding=application/x-tex>A(j,m)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.75em;></span><span class="strut bottom" style=height:1em;vertical-align:-0.25em;></span><span class="base textstyle uncramped"><span class="mord mathit">A</span><span class=mopen>(</span><span class="mord mathit" style=margin-right:0.05724em;>j</span><span class=mpunct>,</span><span class="mord mathit">m</span><span class=mclose>)</span></span></span></span> denotes the time taken by the slowest stage in the <strong>optimal</strong> pipeline between layer 1 and <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=application/x-tex>j</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.65952em;></span><span class="strut bottom" style=height:0.85396em;vertical-align:-0.19444em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.05724em;>j</span></span></span></span> using <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>m</mi></mrow><annotation encoding=application/x-tex>m</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.43056em;></span><span class="strut bottom" style=height:0.43056em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit">m</span></span></span></span> machines</h5><h5><span class=katex><span class=katex-mathml><math><semantics><mrow><mi>T</mi><mo>(</mo><mi>i</mi><mo>→</mo><mi>j</mi><mo separator=true>,</mo><mi>m</mi><mo>)</mo></mrow><annotation encoding=application/x-tex>T(i \rightarrow j, m)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.75em;></span><span class="strut bottom" style=height:1em;vertical-align:-0.25em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.13889em;>T</span><span class=mopen>(</span><span class="mord mathit">i</span><span class=mrel>→</span><span class="mord mathit" style=margin-right:0.05724em;>j</span><span class=mpunct>,</span><span class="mord mathit">m</span><span class=mclose>)</span></span></span></span> denotes the time taken by a single stage spanning layers <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=application/x-tex>i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.65952em;></span><span class="strut bottom" style=height:0.65952em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit">i</span></span></span></span> through <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=application/x-tex>j</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.65952em;></span><span class="strut bottom" style=height:0.85396em;vertical-align:-0.19444em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.05724em;>j</span></span></span></span>, replicated over <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>m</mi></mrow><annotation encoding=application/x-tex>m</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.43056em;></span><span class="strut bottom" style=height:0.43056em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit">m</span></span></span></span> machines</h5><h5><span class=katex><span class=katex-mathml><math><semantics><mrow><mi>T</mi><mo>(</mo><mi>i</mi><mo>→</mo><mi>j</mi><mo separator=true>,</mo><mi>m</mi><mo>)</mo><mo>=</mo><mfrac><mrow><mn>1</mn></mrow><mrow><mi>m</mi></mrow></mfrac><mi>m</mi><mi>a</mi><mi>x</mi><mo>(</mo><msubsup><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mi>i</mi></mrow><mrow><mi>j</mi></mrow></msubsup><msub><mi>T</mi><mi>l</mi></msub><mo separator=true>,</mo><msubsup><mo>∑</mo><mrow><mi>l</mi><mo>=</mo><mi>i</mi></mrow><mrow><mi>j</mi></mrow></msubsup><msubsup><mi>W</mi><mi>l</mi><mi>m</mi></msubsup><mo>)</mo></mrow><annotation encoding=application/x-tex>T(i \rightarrow j, m) = \frac{1}{m} max (\sum_{l=i}^{j} T_l, \sum_{l=i}^{j} W_l^m)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.942572em;></span><span class="strut bottom" style=height:1.287572em;vertical-align:-0.345em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.13889em;>T</span><span class=mopen>(</span><span class="mord mathit">i</span><span class=mrel>→</span><span class="mord mathit" style=margin-right:0.05724em;>j</span><span class=mpunct>,</span><span class="mord mathit">m</span><span class=mclose>)</span><span class=mrel>=</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class=mfrac><span class=vlist><span style=top:0.345em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit">m</span></span></span></span><span style=top:-0.22999999999999998em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style=top:-0.394em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathrm">1</span></span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class="mord mathit">m</span><span class="mord mathit">a</span><span class="mord mathit">x</span><span class=mopen>(</span><span class=mop><span class="op-symbol small-op mop" style=top:-0.0000050000000000050004em;>∑</span><span class=vlist><span style=top:0.3013079999999999em;margin-left:0em;margin-right:0.05em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style=margin-right:0.01968em;>l</span><span class=mrel>=</span><span class="mord mathit">i</span></span></span></span><span style=top:-0.480908em;margin-right:0.05em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style=margin-right:0.05724em;>j</span></span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span><span class=mord><span class="mord mathit" style=margin-right:0.13889em;>T</span><span class=vlist><span style=top:0.15em;margin-right:0.05em;margin-left:-0.13889em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style=margin-right:0.01968em;>l</span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span><span class=mpunct>,</span><span class=mop><span class="op-symbol small-op mop" style=top:-0.0000050000000000050004em;>∑</span><span class=vlist><span style=top:0.3013079999999999em;margin-left:0em;margin-right:0.05em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style=margin-right:0.01968em;>l</span><span class=mrel>=</span><span class="mord mathit">i</span></span></span></span><span style=top:-0.480908em;margin-right:0.05em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style=margin-right:0.05724em;>j</span></span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span><span class=mord><span class="mord mathit" style=margin-right:0.13889em;>W</span><span class=vlist><span style=top:0.2831079999999999em;margin-left:-0.13889em;margin-right:0.05em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style=margin-right:0.01968em;>l</span></span></span><span style=top:-0.363em;margin-right:0.05em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord mathit">m</span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span><span class=mclose>)</span></span></span></span></h5><h5></h5></div><div class=column><h3>DP Formula</h3><h4>Case 1:: Optimal pipeline only has 1 stage</h4><h4><span class=katex><span class=katex-mathml><math><semantics><mrow><mi>A</mi><mo>(</mo><mi>j</mi><mo separator=true>,</mo><mi>m</mi><mo>)</mo><mo>=</mo><mi>T</mi><mo>(</mo><mn>1</mn><mo>→</mo><mi>j</mi><mo separator=true>,</mo><mi>m</mi><mo>)</mo></mrow><annotation encoding=application/x-tex>A(j,m)=T(1\rightarrow j, m)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.75em;></span><span class="strut bottom" style=height:1em;vertical-align:-0.25em;></span><span class="base textstyle uncramped"><span class="mord mathit">A</span><span class=mopen>(</span><span class="mord mathit" style=margin-right:0.05724em;>j</span><span class=mpunct>,</span><span class="mord mathit">m</span><span class=mclose>)</span><span class=mrel>=</span><span class="mord mathit" style=margin-right:0.13889em;>T</span><span class=mopen>(</span><span class="mord mathrm">1</span><span class=mrel>→</span><span class="mord mathit" style=margin-right:0.05724em;>j</span><span class=mpunct>,</span><span class="mord mathit">m</span><span class=mclose>)</span></span></span></span></h4><h4>Case 2:: Optimal pipelien contains multiple stages</h4><p><img src=./img/DP-DP.3101e50e.png alt=DP-DP></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class=aligncenter>2. Layer Partitioning</h1><h3 class=aligncenter>Implementation in Go</h3><div class="vertical-align grid"><div class=column><pre class=language-go><code class=language-go><span class="token keyword">func</span> <span class="token function">A</span><span class="token punctuation">(</span>j<span class="token punctuation">,</span> m <span class="token builtin">int</span><span class="token punctuation">,</span> ms <span class="token operator">*</span>ModelStats<span class="token punctuation">)</span> <span class="token builtin">float64</span> <span class="token punctuation">{</span>
	<span class="token keyword">if</span> j <span class="token operator">==</span> <span class="token number">1</span> <span class="token punctuation">{</span>
		<span class="token keyword">return</span> <span class="token function">T</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> m<span class="token punctuation">,</span> ms<span class="token punctuation">)</span>
	<span class="token punctuation">}</span>
	<span class="token keyword">if</span> m <span class="token operator">==</span> <span class="token number">1</span> <span class="token punctuation">{</span>
		<span class="token keyword">return</span> <span class="token function">T</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> j<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> ms<span class="token punctuation">)</span>
	<span class="token punctuation">}</span>
	globalmin <span class="token operator">:=</span> math<span class="token punctuation">.</span>MaxFloat64
	<span class="token keyword">for</span> i <span class="token operator">:=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> j<span class="token punctuation">;</span> i<span class="token operator">++</span> <span class="token punctuation">{</span>
		localmin <span class="token operator">:=</span> math<span class="token punctuation">.</span>MaxFloat64
		<span class="token keyword">for</span> mp <span class="token operator">:=</span> <span class="token number">1</span><span class="token punctuation">;</span> mp <span class="token operator">&lt;</span> m<span class="token punctuation">;</span> mp<span class="token operator">++</span> <span class="token punctuation">{</span>
			localmax <span class="token operator">:=</span> math<span class="token punctuation">.</span><span class="token function">Max</span><span class="token punctuation">(</span>math<span class="token punctuation">.</span><span class="token function">Max</span><span class="token punctuation">(</span><span class="token function">T</span><span class="token punctuation">(</span>i<span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span> j<span class="token punctuation">,</span> mp<span class="token punctuation">,</span> ms<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">A</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> m<span class="token operator">-</span>mp<span class="token punctuation">,</span> ms<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token function">C</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> ms<span class="token punctuation">)</span><span class="token punctuation">)</span>
			<span class="token keyword">if</span> localmax <span class="token operator">&lt;</span> localmin <span class="token punctuation">{</span>
				localmin <span class="token operator">=</span> localmax
			<span class="token punctuation">}</span>
		<span class="token punctuation">}</span>

		<span class="token keyword">if</span> localmin <span class="token operator">&lt;</span> globalmin <span class="token punctuation">{</span>
			globalmin <span class="token operator">=</span> localmin
		<span class="token punctuation">}</span>
	<span class="token punctuation">}</span>
	<span class="token keyword">return</span> globalmin
<span class="token punctuation">}</span>
</code></pre></div><div class=column><pre class=language-go><code class=language-go><span class="token keyword">func</span> <span class="token function">C</span><span class="token punctuation">(</span>i <span class="token builtin">int</span><span class="token punctuation">,</span> ms <span class="token operator">*</span>ModelStats<span class="token punctuation">)</span> <span class="token builtin">float64</span> <span class="token punctuation">{</span>
	<span class="token keyword">return</span> <span class="token function">float64</span><span class="token punctuation">(</span>ms<span class="token punctuation">.</span>Layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>Size<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token function">float64</span><span class="token punctuation">(</span><span class="token number">50000</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>
<span class="token keyword">func</span> <span class="token function">T</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> j<span class="token punctuation">,</span> m <span class="token builtin">int</span><span class="token punctuation">,</span> ms <span class="token operator">*</span>ModelStats<span class="token punctuation">)</span> <span class="token builtin">float64</span> <span class="token punctuation">{</span>
	TlSum<span class="token punctuation">,</span> WlSum <span class="token operator">:=</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span>
	<span class="token keyword">for</span> i <span class="token operator">:=</span> <span class="token number">0</span><span class="token punctuation">;</span> i <span class="token operator">&lt;</span> <span class="token function">len</span><span class="token punctuation">(</span>ms<span class="token punctuation">.</span>Layers<span class="token punctuation">)</span><span class="token punctuation">;</span> i<span class="token operator">++</span> <span class="token punctuation">{</span>
		TlSum <span class="token operator">=</span> TlSum <span class="token operator">+</span> ms<span class="token punctuation">.</span>Layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>FwdTime <span class="token operator">+</span> ms<span class="token punctuation">.</span>Layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>BackTime
		WlSum <span class="token operator">=</span> WlSum <span class="token operator">+</span> <span class="token function">W</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> m<span class="token punctuation">,</span> ms<span class="token punctuation">)</span>
	<span class="token punctuation">}</span>

	<span class="token keyword">return</span> math<span class="token punctuation">.</span><span class="token function">Max</span><span class="token punctuation">(</span>WlSum<span class="token punctuation">,</span> TlSum<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token function">float64</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span>
<span class="token punctuation">}</span>

<span class="token keyword">func</span> <span class="token function">W</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> m <span class="token builtin">int</span><span class="token punctuation">,</span> ms <span class="token operator">*</span>ModelStats<span class="token punctuation">)</span> <span class="token builtin">float64</span> <span class="token punctuation">{</span>
	<span class="token keyword">return</span> <span class="token number">4.0</span> <span class="token operator">*</span> <span class="token function">float64</span><span class="token punctuation">(</span>m<span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token function">float64</span><span class="token punctuation">(</span>m<span class="token punctuation">)</span> <span class="token operator">*</span> <span class="token function">float64</span><span class="token punctuation">(</span>ms<span class="token punctuation">.</span>Layers<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">.</span>ParamNum<span class="token punctuation">)</span>
<span class="token punctuation">}</span>
<span class="token keyword">func</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>
	ms <span class="token operator">:=</span> <span class="token function">readFromModelSummary</span><span class="token punctuation">(</span><span class="token string">"model.summary"</span><span class="token punctuation">)</span>
	ms <span class="token operator">=</span> <span class="token function">readFromModelPerformance</span><span class="token punctuation">(</span><span class="token string">"model.performance"</span><span class="token punctuation">,</span> ms<span class="token punctuation">)</span>
	fmt<span class="token punctuation">.</span><span class="token function">Printf</span><span class="token punctuation">(</span><span class="token string">"%f\n"</span><span class="token punctuation">,</span> <span class="token function">A</span><span class="token punctuation">(</span><span class="token function">len</span><span class="token punctuation">(</span>ms<span class="token punctuation">.</span>Layers<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> ms<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">}</span>
</code></pre></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><div class="vertical-align grid"><div class=column><h1>3. Work Scheduling</h1><h3>Startup phase</h3><h5>inputs stage emits <code>NOAM</code> minibatches to the pipeline</h5><h5>NOAM = (# machines) / (# machines in input stages)</h5><h3>Steady phase</h3><h5>each stage alternates between forward and backward pass for a minibatch (1F1B)</h5><h3>for stages w/ Data Parallelism config</h3><h5>Round-Robin between multiple GPUs</h5><hr><p>[8]:: Varun Batra &quot;PipeDream&quot; http:://pages.cs.wisc.edu/~shivaram/cs744-slides/cs744-pipedream-varun.pdf</p></div><div class=column><p><img src=./img/PD-8.e4e223bf.png alt=DP-8></p></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1>Effective Learning</h1><div class="vertical-align grid"><div class=column><h2>1. Weight Stashing</h2><h4>Maintain multiple versions of weights</h4><h5>Within a stage, same version of parameters are used for Forward and Backward pass of a given minibatch</h5></div><div class=column><h2>2. Vertical Sync</h2><h4>Full synchronization of parameters</h4><h5>Eliminates the potential inconsistency <em>across stages</em></h5><p>footnote from paper:: impact of vertical sync is negligible, disabled by default</p></div></div></div></section><section slide class=slide :class="text-apple aligncenter"><div class="wrap text-apple aligncenter" wrap=true><h1>Effective Learning</h1><h2>Pipelining w/ Weight Stashing</h2><p><img src=./img/PD-WS.9231c2a3.png alt=PD-WS></p></div></section><section slide class="slide bg-apple aligncenter"><div class=wrap wrap=true><h1>GPipe</h1><h3>Easy Scaling with Micro-Batch Pipeline Parallelism</h3><hr><p>Google</p></div></section><section slide class=slide :class="text-apple size-70"><div class="wrap text-apple size-70" wrap=true><h1>Objectives</h1><h3>GPipe</h3><h4>enables efficient training of large neural networks</h4><h3>PipeDream</h3><h4>optimize “time to target accuracy”</h4></div></section><section slide class=slide :class="text-apple size-70"><div class="wrap text-apple size-70" wrap=true><h1>Interface</h1><h2><span class=katex><span class=katex-mathml><math><semantics><mrow><msub><mi>f</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>f_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.69444em;></span><span class="strut bottom" style=height:0.8888799999999999em;vertical-align:-0.19444em;></span><span class="base textstyle uncramped"><span class=mord><span class="mord mathit" style=margin-right:0.10764em;>f</span><span class=vlist><span style=top:0.15em;margin-right:0.05em;margin-left:-0.10764em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span></span></span></span></h2><h4>Forward Computation Function</h4><h2><span class=katex><span class=katex-mathml><math><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>w_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.43056em;></span><span class="strut bottom" style=height:0.58056em;vertical-align:-0.15em;></span><span class="base textstyle uncramped"><span class=mord><span class="mord mathit" style=margin-right:0.02691em;>w</span><span class=vlist><span style=top:0.15em;margin-right:0.05em;margin-left:-0.02691em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span></span></span></span></h2><h4>Set of Parameters</h4><h2><span class=katex><span class=katex-mathml><math><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub></mrow><annotation encoding=application/x-tex>c_i</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.43056em;></span><span class="strut bottom" style=height:0.58056em;vertical-align:-0.15em;></span><span class="base textstyle uncramped"><span class=mord><span class="mord mathit">c</span><span class=vlist><span style=top:0.15em;margin-right:0.05em;margin-left:0em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit">i</span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span></span></span></span></h2><h4>(optional) Cost Estimation Function</h4></div></section><section slide class=slide :class="text-apple size-70"><div class="wrap text-apple size-70" wrap=true><p><img src=./img/GP-2.d394fbf1.png alt=GP-2></p></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class=aligncenter>Performance 1</h1><div class="vertical-align grid"><div class=column><h2>Re-materialization</h2><blockquote><p>&quot;During forward computation, each accelerator only stores output activations at the partition boundaries. During the backward pass, the <span class=katex><span class=katex-mathml><math><semantics><mrow><mi>k</mi></mrow><annotation encoding=application/x-tex>k</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.69444em;></span><span class="strut bottom" style=height:0.69444em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.03148em;>k</span></span></span></span>-th accelerator recomputes the composite forward function <span class=katex><span class=katex-mathml><math><semantics><mrow><msub><mi>F</mi><mi>k</mi></msub></mrow><annotation encoding=application/x-tex>F_k</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.68333em;></span><span class="strut bottom" style=height:0.83333em;vertical-align:-0.15em;></span><span class="base textstyle uncramped"><span class=mord><span class="mord mathit" style=margin-right:0.13889em;>F</span><span class=vlist><span style=top:0.15em;margin-right:0.05em;margin-left:-0.13889em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord mathit" style=margin-right:0.03148em;>k</span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span></span></span></span> &quot;</p></blockquote><h2>Notations</h2><p><span class=katex><span class=katex-mathml><math><semantics><mrow><mi>N</mi></mrow><annotation encoding=application/x-tex>N</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.68333em;></span><span class="strut bottom" style=height:0.68333em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.10903em;>N</span></span></span></span>:: Mini-batch size<br><span class=katex><span class=katex-mathml><math><semantics><mrow><mi>M</mi></mrow><annotation encoding=application/x-tex>M</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.68333em;></span><span class="strut bottom" style=height:0.68333em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.10903em;>M</span></span></span></span>:: Micro-batch size<br><span class=katex><span class=katex-mathml><math><semantics><mrow><mi>L</mi></mrow><annotation encoding=application/x-tex>L</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.68333em;></span><span class="strut bottom" style=height:0.68333em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit">L</span></span></span></span>:: Number of Layers<br><span class=katex><span class=katex-mathml><math><semantics><mrow><mi>K</mi></mrow><annotation encoding=application/x-tex>K</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.68333em;></span><span class="strut bottom" style=height:0.68333em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.07153em;>K</span></span></span></span>:: Number of Partitions</p></div><div class=column><h3>Peak Activation Memory w/ Re-materialization</h3><h4><span class=katex><span class=katex-mathml><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>N</mi><mo>+</mo><mfrac><mrow><mi>L</mi></mrow><mrow><mi>K</mi></mrow></mfrac><mo>×</mo><mfrac><mrow><mi>N</mi></mrow><mrow><mi>M</mi></mrow></mfrac><mo>)</mo></mrow><annotation encoding=application/x-tex>O(N + \frac{L}{K} \times \frac{N}{M})</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.872331em;></span><span class="strut bottom" style=height:1.217331em;vertical-align:-0.345em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class="mord mathit" style=margin-right:0.10903em;>N</span><span class=mbin>+</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class=mfrac><span class=vlist><span style=top:0.345em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style=margin-right:0.07153em;>K</span></span></span></span><span style=top:-0.22999999999999998em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style=top:-0.394em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit">L</span></span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class=mbin>×</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class=mfrac><span class=vlist><span style=top:0.345em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style=margin-right:0.10903em;>M</span></span></span></span><span style=top:-0.22999999999999998em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style=top:-0.394em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style=margin-right:0.10903em;>N</span></span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class=mclose>)</span></span></span></span></h4><h3>Peak Activation Memory for PipeDream</h3><h4><span class=katex><span class=katex-mathml><math><semantics><mrow><mi>O</mi><mo>(</mo><mi>N</mi><mo>×</mo><mi>L</mi><mo>)</mo></mrow><annotation encoding=application/x-tex>O(N \times L)</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.75em;></span><span class="strut bottom" style=height:1em;vertical-align:-0.25em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class="mord mathit" style=margin-right:0.10903em;>N</span><span class=mbin>×</span><span class="mord mathit">L</span><span class=mclose>)</span></span></span></span></h4></div></div></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h1 class=aligncenter>Performance 2</h1><div class="vertical-align text-quote grid"><div class=column><h2>Bubble Overhead</h2><h4><span class=katex><span class=katex-mathml><math><semantics><mrow><mo>≈</mo><mi>O</mi><mo>(</mo><mfrac><mrow><mi>K</mi><mo>−</mo><mn>1</mn></mrow><mrow><mi>M</mi><mo>+</mo><mi>K</mi><mo>−</mo><mn>1</mn></mrow></mfrac><mo>)</mo></mrow><annotation encoding=application/x-tex>\approx O(\frac{K-1}{M+K-1})</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.872331em;></span><span class="strut bottom" style=height:1.275662em;vertical-align:-0.403331em;></span><span class="base textstyle uncramped"><span class=mrel>≈</span><span class="mord mathit" style=margin-right:0.02778em;>O</span><span class=mopen>(</span><span class="mord reset-textstyle textstyle uncramped"><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span><span class=mfrac><span class=vlist><span style=top:0.345em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle cramped"><span class="mord scriptstyle cramped"><span class="mord mathit" style=margin-right:0.10903em;>M</span><span class=mbin>+</span><span class="mord mathit" style=margin-right:0.07153em;>K</span><span class=mbin>−</span><span class="mord mathrm">1</span></span></span></span><span style=top:-0.22999999999999998em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle textstyle uncramped frac-line"></span></span><span style=top:-0.394em;><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span><span class="reset-textstyle scriptstyle uncramped"><span class="mord scriptstyle uncramped"><span class="mord mathit" style=margin-right:0.07153em;>K</span><span class=mbin>−</span><span class="mord mathrm">1</span></span></span></span><span class=baseline-fix><span class="fontsize-ensurer reset-size5 size5"><span style=font-size:0em;>​</span></span>​</span></span></span><span class="sizing reset-size5 size5 reset-textstyle textstyle uncramped nulldelimiter"></span></span><span class=mclose>)</span></span></span></span></h4><hr><blockquote><p>In our experiments, we found the bubble overhead to be negligible when M ≥ 4 × K. This is also partly because re-computation during the backward pass can be scheduled earlier, without waiting for the gradients from earlier layers.</p></blockquote></div><div class=column><table><thead><tr><th style=text-align:center>TPU</th><th style=text-align:center>AmoebaNet</th><th style=text-align:center>Transformer</th></tr></thead><tbody><tr><td style=text-align:center><span class=katex><span class=katex-mathml><math><semantics><mrow><mi>K</mi><mo>=</mo></mrow><annotation encoding=application/x-tex>K=</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.68333em;></span><span class="strut bottom" style=height:0.68333em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.07153em;>K</span><span class=mrel>=</span></span></span></span></td><td style=text-align:center><span class=katex><span class=katex-mathml><math><semantics><mrow><mn>8</mn></mrow><annotation encoding=application/x-tex>8</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.64444em;></span><span class="strut bottom" style=height:0.64444em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathrm">8</span></span></span></span></td><td style=text-align:center><span class=katex><span class=katex-mathml><math><semantics><mrow><mn>8</mn></mrow><annotation encoding=application/x-tex>8</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.64444em;></span><span class="strut bottom" style=height:0.64444em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathrm">8</span></span></span></span></td></tr><tr><td style=text-align:center><span class=katex><span class=katex-mathml><math><semantics><mrow><mi>M</mi><mo>=</mo></mrow><annotation encoding=application/x-tex>M=</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.68333em;></span><span class="strut bottom" style=height:0.68333em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathit" style=margin-right:0.10903em;>M</span><span class=mrel>=</span></span></span></span></td><td style=text-align:center><span class=katex><span class=katex-mathml><math><semantics><mrow><mn>3</mn><mn>2</mn></mrow><annotation encoding=application/x-tex>32</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.64444em;></span><span class="strut bottom" style=height:0.64444em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathrm">2</span></span></span></span></td><td style=text-align:center><span class=katex><span class=katex-mathml><math><semantics><mrow><mn>3</mn><mn>2</mn></mrow><annotation encoding=application/x-tex>32</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.64444em;></span><span class="strut bottom" style=height:0.64444em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathrm">2</span></span></span></span></td></tr><tr><td style=text-align:center>Speedups on TPUs</td><td style=text-align:center><span class=katex><span class=katex-mathml><math><semantics><mrow><mn>3</mn><mi mathvariant=normal>.</mi><mn>4</mn><mn>8</mn></mrow><annotation encoding=application/x-tex>3.48</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.64444em;></span><span class="strut bottom" style=height:0.64444em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathrm">.</span><span class="mord mathrm">4</span><span class="mord mathrm">8</span></span></span></span></td><td style=text-align:center><span class=katex><span class=katex-mathml><math><semantics><mrow><mn>6</mn><mi mathvariant=normal>.</mi><mn>3</mn></mrow><annotation encoding=application/x-tex>6.3</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.64444em;></span><span class="strut bottom" style=height:0.64444em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathrm">6</span><span class="mord mathrm">.</span><span class="mord mathrm">3</span></span></span></span></td></tr><tr><td style=text-align:center>Speedups on GPU w/out high-speed interconnect</td><td style=text-align:center><span class=katex><span class=katex-mathml><math><semantics><mrow><mn>2</mn><mi mathvariant=normal>.</mi><mn>7</mn></mrow><annotation encoding=application/x-tex>2.7</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.64444em;></span><span class="strut bottom" style=height:0.64444em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathrm">2</span><span class="mord mathrm">.</span><span class="mord mathrm">7</span></span></span></span></td><td style=text-align:center><span class=katex><span class=katex-mathml><math><semantics><mrow><mn>3</mn><mi mathvariant=normal>.</mi><mn>3</mn></mrow><annotation encoding=application/x-tex>3.3</annotation></semantics></math></span><span class=katex-html aria-hidden=true><span class=strut style=height:0.64444em;></span><span class="strut bottom" style=height:0.64444em;vertical-align:0em;></span><span class="base textstyle uncramped"><span class="mord mathrm">3</span><span class="mord mathrm">.</span><span class="mord mathrm">3</span></span></span></span></td></tr></tbody></table><blockquote><p>In contrast, the AmoebaNet model achieves sub-linear speedup due to its imbalanced computation distribution. <cite>3 Performance Analysis</cite></p></blockquote></div></div></div></section><section slide class="slide bg-apple aligncenter"><div class=wrap wrap=true><h1>Comparison btw/ GPipe and PipeDream</h1></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h2>GPipe's comment on PipeDream</h2><blockquote><p>To avoid optimization issues stemming from the weight staleness, PipeDream requires maintaining multiple versioned copies of the model parameters on each accelerator in order to compute the gradient updates accurately, preventing users from scaling to bigger models.</p></blockquote><h2>Aaron's comment on GPipe</h2><blockquote><p>GPipe performs forward passes followed by backward passes for every m minibatches, aggregating weight gradients along the way; it also does not store intermediate state generated during the forward pass needed for the backward pass, instead opting to re-compute them. As a result, it suffers from reduced hardware efficiency due to re-computation overheads and frequent pipeline flushes every m minibatches.</p></blockquote></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><p><img src=./img/aaron-on-gpipe.e1c00eaf.png alt=aaron-on-gpipe></p></div></section><section slide class=slide :class=text-apple><div class="wrap text-apple" wrap=true><h2>Aaron's Performance Comparison</h2><blockquote class=text-quote><p>&quot;We compare training GNMT-16 using PipeDream and our implementation of GPipe using 16 GPUs on Cluster-A and Cluster-B. GPipe does not provide an algorithm for partitioning work across stages, so we use the same partitions as PipeDream. GPipe also does not provide an algorithm for how many items should be permitted into the “pipeline” (pipeline depth). When we set the pipeline depth to be equivalent to “NOAM” in PipeDream, GPipe experiences <strong>55% and 71% throughput slowdowns</strong> compared to PipeDream on Cluster- A and Cluster-B, respectively. Setting the pipeline depth for GPipe to the largest number that does not cause an out-of-memory exception, leads to throughput slowdowns of 35% and 42% on Cluster-A and Cluster-B, respectively. Note that, unlike PipeDream, GPipe may suffer from reduced statistical efficiency because each weight is updated only when the pipeline is flushed, but we did not explicitly measure this.&quot; <cite>Aaron Harlap Dissertation</cite></p></blockquote></div></section><section slide class="slide bg-black-blue text-apple aligncenter" image="https://rongyi.io/wp-content/uploads/2019/05/angelwing.jpg .dark"><span class="background dark" style="background-image:url('https://rongyi.io/wp-content/uploads/2019/05/angelwing.jpg')"></span><div class=wrap wrap=true><h2 class=aligncenter>References</h2><div class="vertical-align grid"><div class=column><p>[1]:: Dean, Jeffrey, et al. &quot;Large scale distributed deep networks.&quot;<br>[2]:: Saliya Ekanayake. &quot;Model Parallelism in Deep Learning is NOT What You Think&quot;<br>[3]:: Aaron Harlap, et al. &quot;PipeDream:: Fast and Efficient Parallel DNN Training&quot;<br>[4]:: Alex Krizhevsky. &quot;One weird trick for parallelizing convolutional neural networks&quot;<br>[5]:: Anand Jayarajan, et al. &quot;Priority-based Parameter Propagation for Distributed DNN Training&quot;<br>[6]:: Yanping Huang, et al. &quot;GPipe:: Efficient Training of Giant Neural Networks using Pipeline Parallelism&quot;<br>[7]:: Tal Ben-Nun, Torsten Hoefler. &quot;Demystifying Parallel and Distributed Deep Learning:: An In-Depth Concurrency Analysis&quot;<br>[8]:: Varun Batra &quot;PipeDream&quot; http:://pages.cs.wisc.edu/~shivaram/cs744-slides/cs744-pipedream-varun.pdf<br>[9]:: Aaron Harlap &quot;Dissertation:: Improving ML applications in shared computing environments&quot;</p></div><div class=column><p><a href=https://github.com/LER0ever/Presentations class="button animated delay-1s fadeInUp alignright" target=_blank><i class="fa fa-github"></i> Source code of this presentation</a></p></div></div></div></section></article></div><script src=//cdn.staticfile.org/echarts/4.1.0-release/echarts.min.js></script><script src=//cdn.staticfile.org/mermaid/8.0.0/mermaid.min.js></script><script>mermaid.startOnLoad = false;</script><script src=https://www.echartsjs.com/asset/theme/infographic.js></script><script>window.pluginsOptions = {"echarts":{"theme":"infographic"},"mermaid":{"theme":"forest"}}



document.addEventListener('DOMContentLoaded', () => {
    const ws = new WebSlides({
        loop: false
    })
    window.wsInstance = ws;
}, false)</script><script src=./js/chunk-vendors.3e6fe9a4.js></script><script src=./js/190809.77837544.js></script></body></html>